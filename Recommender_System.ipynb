{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movies= pd.read_csv(\"movies.dat\", sep= '::', header=None, engine='python', encoding='latin-1')\n",
    "users= pd.read_csv(\"users.dat\", sep= '::', header=None, engine='python', encoding='latin-1')\n",
    "ratings= pd.read_csv(\"ratings.dat\", sep= '::', header=None, engine='python', encoding='latin-1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_set= pd.read_csv(\"u1.base\", delimiter='\\t', header=None)\n",
    "#training_set.loc[0:1000]\n",
    "training_set= np.array(training_set, dtype='int')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_set= pd.read_csv(\"u1.test\", delimiter='\\t')\n",
    "test_set= np.array(test_set, dtype='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_users = int(max(max(training_set[:,0]), max(test_set[:,0])))\n",
    "nb_movies= int(max(max(training_set[:,1]), max(test_set[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert(data):\n",
    "    new_data=[]\n",
    "    for id_users in range(1, nb_users+1):\n",
    "        id_movies= data[:,1][data[:,0]== id_users]\n",
    "        id_ratings= data[:,2][data[:,0]== id_users]\n",
    "        ratings= np.zeros(nb_movies)\n",
    "        ratings[id_movies-1] = id_ratings\n",
    "        new_data.append(list(ratings))\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_set= convert(training_set)\n",
    "test_set= convert(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_set= torch.FloatTensor(training_set)\n",
    "test_set= torch.FloatTensor(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0\n",
       " 5\n",
       " 0\n",
       "â‹® \n",
       " 0\n",
       " 0\n",
       " 0\n",
       "[torch.FloatTensor of size 1682]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set[942]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SAE(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super(SAE, self).__init__()\n",
    "        self.fc1 = nn.Linear(nb_movies, 20)\n",
    "        self.fc2 = nn.Linear(20, 10)\n",
    "        self.fc3 = nn.Linear(10, 20)\n",
    "        self.fc4 = nn.Linear(20, nb_movies)\n",
    "        self.activation = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.activation(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "sae = SAE()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.RMSprop(sae.parameters(), lr = 0.01, weight_decay = 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 loss: 1.77089828445\n",
      "epoch: 2 loss: 1.09681043589\n",
      "epoch: 3 loss: 1.05338885665\n",
      "epoch: 4 loss: 1.0382936281\n",
      "epoch: 5 loss: 1.03081596785\n",
      "epoch: 6 loss: 1.026729371\n",
      "epoch: 7 loss: 1.02373490902\n",
      "epoch: 8 loss: 1.02212831838\n",
      "epoch: 9 loss: 1.02083409241\n",
      "epoch: 10 loss: 1.01972154101\n",
      "epoch: 11 loss: 1.01890989071\n",
      "epoch: 12 loss: 1.01835399412\n",
      "epoch: 13 loss: 1.01785755697\n",
      "epoch: 14 loss: 1.01770265967\n",
      "epoch: 15 loss: 1.0171365729\n",
      "epoch: 16 loss: 1.01696343425\n",
      "epoch: 17 loss: 1.01649132176\n",
      "epoch: 18 loss: 1.01663156366\n",
      "epoch: 19 loss: 1.01628946866\n",
      "epoch: 20 loss: 1.01638136435\n",
      "epoch: 21 loss: 1.01580046812\n",
      "epoch: 22 loss: 1.01614092128\n",
      "epoch: 23 loss: 1.01605264242\n",
      "epoch: 24 loss: 1.01605236692\n",
      "epoch: 25 loss: 1.01563819699\n",
      "epoch: 26 loss: 1.01568452377\n",
      "epoch: 27 loss: 1.0154643853\n",
      "epoch: 28 loss: 1.01523438515\n",
      "epoch: 29 loss: 1.01281378725\n",
      "epoch: 30 loss: 1.01172912253\n",
      "epoch: 31 loss: 1.00945305397\n",
      "epoch: 32 loss: 1.00919607222\n",
      "epoch: 33 loss: 1.00577275934\n",
      "epoch: 34 loss: 1.00482598386\n",
      "epoch: 35 loss: 1.00159243977\n",
      "epoch: 36 loss: 0.999836994511\n",
      "epoch: 37 loss: 0.997636548216\n",
      "epoch: 38 loss: 0.994997513724\n",
      "epoch: 39 loss: 0.992620301038\n",
      "epoch: 40 loss: 0.994512249571\n",
      "epoch: 41 loss: 0.990591586247\n",
      "epoch: 42 loss: 0.991000325698\n",
      "epoch: 43 loss: 0.984970997584\n",
      "epoch: 44 loss: 0.985610323236\n",
      "epoch: 45 loss: 0.979838793762\n",
      "epoch: 46 loss: 0.98547700798\n",
      "epoch: 47 loss: 0.979478068602\n",
      "epoch: 48 loss: 0.978898467149\n",
      "epoch: 49 loss: 0.975101965067\n",
      "epoch: 50 loss: 0.97563185003\n",
      "epoch: 51 loss: 0.97362824813\n",
      "epoch: 52 loss: 0.97866831416\n",
      "epoch: 53 loss: 0.976630844501\n",
      "epoch: 54 loss: 0.971080561034\n",
      "epoch: 55 loss: 0.968836175322\n",
      "epoch: 56 loss: 0.968244051284\n",
      "epoch: 57 loss: 0.968704098395\n",
      "epoch: 58 loss: 0.964443797359\n",
      "epoch: 59 loss: 0.963927945328\n",
      "epoch: 60 loss: 0.961060343946\n",
      "epoch: 61 loss: 0.956712783531\n",
      "epoch: 62 loss: 0.959685670425\n",
      "epoch: 63 loss: 0.956929491494\n",
      "epoch: 64 loss: 0.956983387862\n",
      "epoch: 65 loss: 0.954302209783\n",
      "epoch: 66 loss: 0.956788215338\n",
      "epoch: 67 loss: 0.955213389845\n",
      "epoch: 68 loss: 0.955406287952\n",
      "epoch: 69 loss: 0.951923786781\n",
      "epoch: 70 loss: 0.952476102952\n",
      "epoch: 71 loss: 0.950125944951\n",
      "epoch: 72 loss: 0.950356594409\n",
      "epoch: 73 loss: 0.947162583953\n",
      "epoch: 74 loss: 0.949430283644\n",
      "epoch: 75 loss: 0.946416238374\n",
      "epoch: 76 loss: 0.94709929751\n",
      "epoch: 77 loss: 0.944211542491\n",
      "epoch: 78 loss: 0.944840106871\n",
      "epoch: 79 loss: 0.941490615487\n",
      "epoch: 80 loss: 0.946050433454\n",
      "epoch: 81 loss: 0.946213250463\n",
      "epoch: 82 loss: 0.945486464187\n",
      "epoch: 83 loss: 0.944444010465\n",
      "epoch: 84 loss: 0.945574757019\n",
      "epoch: 85 loss: 0.942767052061\n",
      "epoch: 86 loss: 0.942339584969\n",
      "epoch: 87 loss: 0.939981751902\n",
      "epoch: 88 loss: 0.941590606098\n",
      "epoch: 89 loss: 0.939321372642\n",
      "epoch: 90 loss: 0.940478439589\n",
      "epoch: 91 loss: 0.937743946138\n",
      "epoch: 92 loss: 0.938453945238\n",
      "epoch: 93 loss: 0.936858824415\n",
      "epoch: 94 loss: 0.93724449245\n",
      "epoch: 95 loss: 0.935738185403\n",
      "epoch: 96 loss: 0.936942774626\n",
      "epoch: 97 loss: 0.935431846751\n",
      "epoch: 98 loss: 0.936843104633\n",
      "epoch: 99 loss: 0.933798140649\n",
      "epoch: 100 loss: 0.935473907533\n",
      "epoch: 101 loss: 0.933501780403\n",
      "epoch: 102 loss: 0.934189204855\n",
      "epoch: 103 loss: 0.932468182822\n",
      "epoch: 104 loss: 0.933227813949\n",
      "epoch: 105 loss: 0.932318769148\n",
      "epoch: 106 loss: 0.932990174431\n",
      "epoch: 107 loss: 0.931985804491\n",
      "epoch: 108 loss: 0.932801579436\n",
      "epoch: 109 loss: 0.931500819905\n",
      "epoch: 110 loss: 0.931676776102\n",
      "epoch: 111 loss: 0.930420766334\n",
      "epoch: 112 loss: 0.935818493946\n",
      "epoch: 113 loss: 0.931062418135\n",
      "epoch: 114 loss: 0.930313802019\n",
      "epoch: 115 loss: 0.929771000037\n",
      "epoch: 116 loss: 0.929807428418\n",
      "epoch: 117 loss: 0.928569990713\n",
      "epoch: 118 loss: 0.92883160589\n",
      "epoch: 119 loss: 0.927759763113\n",
      "epoch: 120 loss: 0.928472553204\n",
      "epoch: 121 loss: 0.927606984184\n",
      "epoch: 122 loss: 0.927604158113\n",
      "epoch: 123 loss: 0.926909329425\n",
      "epoch: 124 loss: 0.927264117498\n",
      "epoch: 125 loss: 0.926494130981\n",
      "epoch: 126 loss: 0.926152462987\n",
      "epoch: 127 loss: 0.925808073715\n",
      "epoch: 128 loss: 0.9256863864\n",
      "epoch: 129 loss: 0.925261128892\n",
      "epoch: 130 loss: 0.925039193822\n",
      "epoch: 131 loss: 0.924244862437\n",
      "epoch: 132 loss: 0.924406467629\n",
      "epoch: 133 loss: 0.92403068441\n",
      "epoch: 134 loss: 0.924132089627\n",
      "epoch: 135 loss: 0.923407712973\n",
      "epoch: 136 loss: 0.923650145826\n",
      "epoch: 137 loss: 0.923181618969\n",
      "epoch: 138 loss: 0.924150658126\n",
      "epoch: 139 loss: 0.925171447333\n",
      "epoch: 140 loss: 0.92856051752\n",
      "epoch: 141 loss: 0.926591072309\n",
      "epoch: 142 loss: 0.926327112257\n",
      "epoch: 143 loss: 0.925145194629\n",
      "epoch: 144 loss: 0.922353836299\n",
      "epoch: 145 loss: 0.920831613123\n",
      "epoch: 146 loss: 0.92168793724\n",
      "epoch: 147 loss: 0.920454058629\n",
      "epoch: 148 loss: 0.92154773102\n",
      "epoch: 149 loss: 0.92031091627\n",
      "epoch: 150 loss: 0.920680296981\n",
      "epoch: 151 loss: 0.919547140385\n",
      "epoch: 152 loss: 0.920043511341\n",
      "epoch: 153 loss: 0.91924565265\n",
      "epoch: 154 loss: 0.919285188863\n",
      "epoch: 155 loss: 0.918570097157\n",
      "epoch: 156 loss: 0.919048972924\n",
      "epoch: 157 loss: 0.91874250559\n",
      "epoch: 158 loss: 0.918960628156\n",
      "epoch: 159 loss: 0.918102498884\n",
      "epoch: 160 loss: 0.918307581164\n",
      "epoch: 161 loss: 0.917810856577\n",
      "epoch: 162 loss: 0.917931561979\n",
      "epoch: 163 loss: 0.917336037656\n",
      "epoch: 164 loss: 0.917959460893\n",
      "epoch: 165 loss: 0.917206588921\n",
      "epoch: 166 loss: 0.917281967074\n",
      "epoch: 167 loss: 0.916623183202\n",
      "epoch: 168 loss: 0.916751553479\n",
      "epoch: 169 loss: 0.916557836752\n",
      "epoch: 170 loss: 0.916465323149\n",
      "epoch: 171 loss: 0.916175354962\n",
      "epoch: 172 loss: 0.916213297402\n",
      "epoch: 173 loss: 0.915798815423\n",
      "epoch: 174 loss: 0.916166191826\n",
      "epoch: 175 loss: 0.915559701547\n",
      "epoch: 176 loss: 0.915806250798\n",
      "epoch: 177 loss: 0.915331692139\n",
      "epoch: 178 loss: 0.915507174259\n",
      "epoch: 179 loss: 0.915297920583\n",
      "epoch: 180 loss: 0.915114736044\n",
      "epoch: 181 loss: 0.914629356431\n",
      "epoch: 182 loss: 0.915505951161\n",
      "epoch: 183 loss: 0.914664342269\n",
      "epoch: 184 loss: 0.914713899941\n",
      "epoch: 185 loss: 0.914097751263\n",
      "epoch: 186 loss: 0.91427754389\n",
      "epoch: 187 loss: 0.913827623208\n",
      "epoch: 188 loss: 0.913964363882\n",
      "epoch: 189 loss: 0.91388670463\n",
      "epoch: 190 loss: 0.913759304557\n",
      "epoch: 191 loss: 0.913417687661\n",
      "epoch: 192 loss: 0.913490913806\n",
      "epoch: 193 loss: 0.91326292252\n",
      "epoch: 194 loss: 0.913009333773\n",
      "epoch: 195 loss: 0.912842227102\n",
      "epoch: 196 loss: 0.912994203263\n",
      "epoch: 197 loss: 0.91245637102\n",
      "epoch: 198 loss: 0.912730734316\n",
      "epoch: 199 loss: 0.912480185699\n",
      "epoch: 200 loss: 0.91227121807\n"
     ]
    }
   ],
   "source": [
    "nb_epoch = 200\n",
    "for epoch in range(1, nb_epoch + 1):\n",
    "    train_loss = 0\n",
    "    s = 0.\n",
    "    for id_user in range(nb_users):\n",
    "        input = Variable(training_set[id_user]).unsqueeze(0)\n",
    "        target = input.clone()\n",
    "        if torch.sum(target.data > 0) > 0:\n",
    "            output = sae(input)\n",
    "            target.require_grad = False\n",
    "            output[target == 0] = 0\n",
    "            loss = criterion(output, target)\n",
    "            mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)\n",
    "            loss.backward()\n",
    "            train_loss += np.sqrt(loss.data[0]*mean_corrector)\n",
    "            s += 1.\n",
    "            optimizer.step()\n",
    "    print('epoch: '+str(epoch)+' loss: '+str(train_loss/s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n",
      "test loss: 0.949634824102\n"
     ]
    }
   ],
   "source": [
    "nb_epoch = 200\n",
    "for epoch in range(1, nb_epoch + 1):\n",
    "    test_loss = 0\n",
    "    s = 0.\n",
    "    for id_user in range(nb_users):\n",
    "        input = Variable(training_set[id_user]).unsqueeze(0)\n",
    "        target = Variable(test_set[id_user])\n",
    "        if torch.sum(target.data > 0) > 0:\n",
    "            output = sae(input)\n",
    "            target.require_grad = False\n",
    "            output[target == 0] = 0\n",
    "            loss = criterion(output, target)\n",
    "            mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)\n",
    "            test_loss += np.sqrt(loss.data[0]*mean_corrector)\n",
    "            s += 1.\n",
    "    print('test loss: '+str(test_loss/s))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
